{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Abhishek Thakur Book Concept<center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter - Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Topic Covered:\n",
    "    * VarianceThreshold method\n",
    "    * Correlation method\n",
    "    * Univariate feature selection(smart way)\n",
    "    * Greedy Method\n",
    "        * Exact greedy\n",
    "        * RFE \n",
    "        * forward feature selection\n",
    "    * Model Based Selection\n",
    "        * Using our own method\n",
    "        * Using Scikit learn SelectFromModel\n",
    "        * Using scikit-learn pipeline method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selction is **\"the\"** most critical step in Feature Engineering Task. We have to be very careful when we select feature using techniques.\n",
    "\n",
    "**Why we need:**\n",
    "    \n",
    "   Let say we have thousands of feature most of them may useless for our model. So for better model preparation we have to select some of them and vice versa remove most of them. One more issue that may occur i.e well know problem `Curse of dimensionality` which is nothing but when we have lot of features, we must also have a lot of training samples to captures all features, and it also takes lot of training time (even some distance metric like Euclidean wouldn't give the actual distance in high dim space.This is also be one of reason why we do feature selection.\n",
    "   2nd Reason we just want those which are useful or add some value to the target or my object.\n",
    "   \n",
    "* Now discuss what are the various way to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method - 1:Constant Feature Removing**\n",
    "   - Simplest Approach:\n",
    "        Very first step remove features which have ***`very low variance`***(these feature are constant features that doesn't add anything to model and unneccesarily increase the model training complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6152</td>\n",
       "      <td>22260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8779</td>\n",
       "      <td>21060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3040</td>\n",
       "      <td>23961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6405</td>\n",
       "      <td>24901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6624</td>\n",
       "      <td>23473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7644</td>\n",
       "      <td>25467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2\n",
       "0       6152      22260\n",
       "1       8779      21060\n",
       "2       3040      23961\n",
       "3       6405      24901\n",
       "4       6624      23473\n",
       "5       7644      25467"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scikit-learn provide VarianceThreshold that does this job precisely\n",
    "features = {\n",
    "    \n",
    "    'ID': np.random.randint(1000, 10010, 22),\n",
    "    'Sales_price': np.random.randint(10000, 40000, 22),\n",
    "    'N_Item': np.random.randint(10,11,22)\n",
    "}\n",
    "\n",
    "#craete small fake dataframe for demonstration purpose\n",
    "\n",
    "df = pd.DataFrame(features)\n",
    "\n",
    "#removing features which have variance less than 0.2 .\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "var_thresh = VarianceThreshold(threshold = 0.2)\n",
    "\n",
    "#remove features whose variance close to 0.(constant features)\n",
    "\n",
    "transformed_data = var_thresh.fit_transform(df)\n",
    "\n",
    "df2 = pd.DataFrame(transformed_data,\n",
    "             columns = [ f\"feature_{i}\" for i in range(1, transformed_data.shape[1]+1) ]\n",
    ")\n",
    "df2.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature_3 has been removed which have variance less than 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method - 2:(Using Pearson Correlation)**\n",
    "   - Approach:\n",
    "   \n",
    "        Features which have ***`very high correlation`***, remove them. Let say we've two features in Dataset which are highly correlated then remove one of them and retain only one for modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedIncsqrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>2.885342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>2.881215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>2.693956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>2.375521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>1.961173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  MedIncsqrt  \n",
       "0    -122.23    2.885342  \n",
       "1    -122.22    2.881215  \n",
       "2    -122.24    2.693956  \n",
       "3    -122.25    2.375521  \n",
       "4    -122.25    1.961173  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "#fetching a dataset which gives us dictionary \n",
    "#consist the information about data\n",
    "data = fetch_california_housing()\n",
    "\n",
    "X = data[\"data\"]\n",
    "col_names = data[\"feature_names\"]\n",
    "y = data['target']\n",
    "\n",
    "#convert it into pandas dataframe\n",
    "\n",
    "df = pd.DataFrame(X, columns = col_names)\n",
    "df.head()\n",
    "\n",
    "#creating a highly correlated feature\n",
    "\n",
    "df.loc[: , 'MedIncsqrt'] = df.MedInc.apply(lambda m : np.sqrt(m))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedIncsqrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MedInc</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.119034</td>\n",
       "      <td>0.326895</td>\n",
       "      <td>-0.062040</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.018766</td>\n",
       "      <td>-0.079809</td>\n",
       "      <td>-0.015176</td>\n",
       "      <td>0.984329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HouseAge</th>\n",
       "      <td>-0.119034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.153277</td>\n",
       "      <td>-0.077747</td>\n",
       "      <td>-0.296244</td>\n",
       "      <td>0.013191</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>-0.108197</td>\n",
       "      <td>-0.132797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveRooms</th>\n",
       "      <td>0.326895</td>\n",
       "      <td>-0.153277</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.847621</td>\n",
       "      <td>-0.072213</td>\n",
       "      <td>-0.004852</td>\n",
       "      <td>0.106389</td>\n",
       "      <td>-0.027540</td>\n",
       "      <td>0.326688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveBedrms</th>\n",
       "      <td>-0.062040</td>\n",
       "      <td>-0.077747</td>\n",
       "      <td>0.847621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.066197</td>\n",
       "      <td>-0.006181</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>0.013344</td>\n",
       "      <td>-0.066910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Population</th>\n",
       "      <td>0.004834</td>\n",
       "      <td>-0.296244</td>\n",
       "      <td>-0.072213</td>\n",
       "      <td>-0.066197</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069863</td>\n",
       "      <td>-0.108785</td>\n",
       "      <td>0.099773</td>\n",
       "      <td>0.018415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveOccup</th>\n",
       "      <td>0.018766</td>\n",
       "      <td>0.013191</td>\n",
       "      <td>-0.004852</td>\n",
       "      <td>-0.006181</td>\n",
       "      <td>0.069863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.015266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latitude</th>\n",
       "      <td>-0.079809</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.106389</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>-0.108785</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.924664</td>\n",
       "      <td>-0.084303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longitude</th>\n",
       "      <td>-0.015176</td>\n",
       "      <td>-0.108197</td>\n",
       "      <td>-0.027540</td>\n",
       "      <td>0.013344</td>\n",
       "      <td>0.099773</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>-0.924664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedIncsqrt</th>\n",
       "      <td>0.984329</td>\n",
       "      <td>-0.132797</td>\n",
       "      <td>0.326688</td>\n",
       "      <td>-0.066910</td>\n",
       "      <td>0.018415</td>\n",
       "      <td>0.015266</td>\n",
       "      <td>-0.084303</td>\n",
       "      <td>-0.015569</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  \\\n",
       "MedInc      1.000000 -0.119034  0.326895  -0.062040    0.004834  0.018766   \n",
       "HouseAge   -0.119034  1.000000 -0.153277  -0.077747   -0.296244  0.013191   \n",
       "AveRooms    0.326895 -0.153277  1.000000   0.847621   -0.072213 -0.004852   \n",
       "AveBedrms  -0.062040 -0.077747  0.847621   1.000000   -0.066197 -0.006181   \n",
       "Population  0.004834 -0.296244 -0.072213  -0.066197    1.000000  0.069863   \n",
       "AveOccup    0.018766  0.013191 -0.004852  -0.006181    0.069863  1.000000   \n",
       "Latitude   -0.079809  0.011173  0.106389   0.069721   -0.108785  0.002366   \n",
       "Longitude  -0.015176 -0.108197 -0.027540   0.013344    0.099773  0.002476   \n",
       "MedIncsqrt  0.984329 -0.132797  0.326688  -0.066910    0.018415  0.015266   \n",
       "\n",
       "            Latitude  Longitude  MedIncsqrt  \n",
       "MedInc     -0.079809  -0.015176    0.984329  \n",
       "HouseAge    0.011173  -0.108197   -0.132797  \n",
       "AveRooms    0.106389  -0.027540    0.326688  \n",
       "AveBedrms   0.069721   0.013344   -0.066910  \n",
       "Population -0.108785   0.099773    0.018415  \n",
       "AveOccup    0.002366   0.002476    0.015266  \n",
       "Latitude    1.000000  -0.924664   -0.084303  \n",
       "Longitude  -0.924664   1.000000   -0.015569  \n",
       "MedIncsqrt -0.084303  -0.015569    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get correlation metrics(pearson)\n",
    "\n",
    "df.corr(method =\"pearson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here MedInc and MedIncsqrt are highly correlated with pearson score 0.98 approx. So we have to remove one of the feature from df.\n",
    "\n",
    "NOTE: **`Correlation and Causation are different`** i.e. if two features are correlated it is not meaning that one causes the other for that purpose we've to look at the **causion Models** which is separate statistical analysis than correlation Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method - 3:(Univariate Feature Selection)**\n",
    "   - Approach:\n",
    "   \n",
    "       Try to get the score of each feature against a target. Most popular methods are as follows :-\n",
    "        * chi^2\n",
    "        * Anova\n",
    "        * F-test\n",
    "        * Mutual Information\n",
    "        \n",
    "    There are two ways of using this in scikit-learn that is below:-\n",
    "       * SelectKBest: Select Top k features.\n",
    "       * SeletPercentile: Slect top features based on percentage.\n",
    " \n",
    "* NOTE:\n",
    "     * if we know assumption of Anova/Chi2/etc that is okay then we know where to apply otherwise using SelectKBest/SelectPercentile carelessly might throw out many features for the wrong reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ufs = UnivariateFeatureSelection(problem_type = 'regression', \\n        n_feature = 0.1,\\n        scoring = f_regresion\\n)\\n\\nufs.fit(X,y)\\nX_transformed = ufs.transform(X)\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#custom wrapper to perform univariate feature selection \n",
    "#for \"almost\" any new problem\n",
    "\n",
    "class UnivariateFeatureSelection:\n",
    "    \n",
    "    def __init__(self, problem_type, n_feature, scoring):\n",
    "        \n",
    "        #for classification\n",
    "        if problem_type == 'classification':\n",
    "            valid_scoring = {\n",
    "                \"f_classif\" : f_classif,\n",
    "                \"chi2\" : chi2,\n",
    "                \"mutual_info_classif\" : mutual_info_classif\n",
    "            }\n",
    "       #for regression\n",
    "        else:\n",
    "            valid_scoring = {\n",
    "                \"f_regression\" : f_regression,\n",
    "                \"mutual_info_regression\" : mutual_info_regression\n",
    "            }\n",
    "        if scoring not in valid_scoring:\n",
    "            raise Exception(\"Invalid Scoring Method\")\n",
    "        \n",
    "        #if i want exact number of feature like 5,6 features  \n",
    "        if isinstance(n_feature, int):\n",
    "            self.selection = SelectKBest(valid_scoring[scoring],\n",
    "                                k = n_feature\n",
    "                    )\n",
    "            \n",
    "        #if i want as percentage of feature \n",
    "        elif isinstance(n_feature, float):\n",
    "            self.selection = SelectPercentile(\n",
    "                        valid_scoring[scoring],\n",
    "                        percentile = n_feature * 100\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\"invalid type of feature\")\n",
    "    \n",
    "    #fitting my data \n",
    "    def fit(self, X, y):\n",
    "        return self.selection.fit(X, y)\n",
    "    \n",
    "    #transforming my data \n",
    "    def transform(self):\n",
    "        return self.selection.transform(X)\n",
    "    \n",
    "    #fit_transform in single line \n",
    "    def fit_transform(self):\n",
    "        return self.selection.fit_transform(X, y)\n",
    "    \n",
    "#how to use this wrapper effectively\n",
    "#Code -> \n",
    "\n",
    "\"\"\"ufs = UnivariateFeatureSelection(problem_type = 'regression', \n",
    "        n_feature = 0.1,\n",
    "        scoring = f_regresion\n",
    ")\n",
    "\n",
    "ufs.fit(X,y)\n",
    "X_transformed = ufs.transform(X)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NOTE:\n",
    "    * ***Chi2*** test suitable for only non-negative dataset.\n",
    "    * This technique pretty much good when we build NLP model besded on TF-IDF/BOW model.\n",
    "    * When use this feature selection method it is better to t start with less features to create rather than 100 of features in very first place.\n",
    "    * However this is not a best approach but on some extent it works well for some poblems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method - 4:(Greedy feature Selection)**\n",
    "   - Approach:\n",
    "   \n",
    "       Since univariate feature selection may or may not work well. That's is why people refer **model based feature selection Approach**.\n",
    "       Steps in Greedy:\n",
    "           i) Choose the model first.\n",
    "           ii)Select the loss/scoring function.\n",
    "           iii) Iterative approach evaluate each feature and add it to the list of \"good\" feature iff it imporoves the previous score/loss.\n",
    "           \n",
    "* NOTE: \n",
    "    only disadvantages is that we need to fit the model each time when we evaluate feature. So time complexity get increased. \n",
    "    Moreover there is a high chance of overfitting if this feature selection method we did not use properly.(Why?)\n",
    "    And Also this is not exact forward feature selection because complexity of forward feature selection may be high but greedy doing job smartly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom wrapper for greedy feature selection\n",
    "#for almost any problem\n",
    "class GreedyFeatureSelection:\n",
    "    \n",
    "    #calculate AUC_score using linear model(we can select any model) \n",
    "    def evaluate_score(self, X, y):\n",
    "        model = linear_model.LogisticRegression()\n",
    "        model.fit(X, y)\n",
    "        predictions = model.predict_proba(X)[:, 1]\n",
    "        auc_scores = metrics.roc_auc_score(y, predictions)\n",
    "        return auc_scores\n",
    "        \n",
    "    #actual method for feature selection\n",
    "    def feature_selection(self, X, y):\n",
    "        \n",
    "        #good features and best scores list\n",
    "        good_features = []\n",
    "        best_scores = []\n",
    "        \n",
    "        #number of features in dataset\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        \n",
    "        #infinite iterative loop \n",
    "        while True:\n",
    "            \n",
    "            #for every roud current feature as None and best_score as 0.\n",
    "            current_feature = None \n",
    "            best_score = 0\n",
    "            \n",
    "            #iterative approach for selcting single feature\n",
    "            for feature in range(n_features):\n",
    "                \n",
    "                #if feature aleady in good_feature list skip that one\n",
    "                if feature in good_features:\n",
    "                    continue\n",
    "                 \n",
    "                #create list that includes all features which are already good + current one\n",
    "                select_features = good_features + [feature]\n",
    "                \n",
    "                #prepare data\n",
    "                X_train = X[ : , select_features]\n",
    "                \n",
    "                #get auc_roc score\n",
    "                score = self.evaluate_score(X_train, y)\n",
    "\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    current_feature = feature\n",
    "                    \n",
    "          # add it to good feature list\n",
    "            if current_feature != None:\n",
    "                good_features.append(feature)\n",
    "                best_scores.append(best_score)\n",
    "                \n",
    "          # if in previous round my score wouldn't incearse than exit while loop  \n",
    "            if len(best_scores) > 2:\n",
    "                if best_scores[-2] > best_scores[-1]:\n",
    "                    break\n",
    "                \n",
    "         \n",
    "        #why removing last feature ? becoz previous score doesn't improove\n",
    "        return best_scores[:-1], good_features[:-1]\n",
    "            \n",
    "    \n",
    "    #call function call the class on these parameter\n",
    "    def __call__(self, X, y):\n",
    "        scores, features = self.feature_selection(X, y)\n",
    "        return X[:, features], scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100)\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "#demo check for above \n",
    "if __name__ ==\"__main__\":\n",
    "    #Generate a random n-class classification problem.\n",
    "    X, y = make_classification(n_samples = 1000,\n",
    "                        n_features = 100,\n",
    "                        n_classes = 2)\n",
    "    X_transformed, scores = GreedyFeatureSelection()(X,y)\n",
    "    print(X.shape)\n",
    "    print(X_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method - 4:(Another Greedy mmethod - Recursive Feature Elimination(RFE))**\n",
    "   - Approach:\n",
    "       instead of selecting features one by one, we are removing features one by one.\n",
    "       - we start with all features and remove one feature in every iteration that provide the least score/loss to the model.\n",
    "     Challange is, how to know which feature will give us least score ?\n",
    "           - in case of linear SVM or logistic regression using the coefficient which is close to 0 are less imporatnat and in tree based approach by knowing the feature importance.\n",
    "           \n",
    "           \n",
    "* NOTE: \n",
    "   * Coefficient is more +ve which important for +ve class and Coefficient which are more -ve important for -ve class. \n",
    "   * we have the ability to select the n_features that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n"
     ]
    }
   ],
   "source": [
    "#get data for demonstration purpose \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "#fetch_california_housing \n",
    "data = fetch_california_housing()\n",
    "X = data['data']\n",
    "col_names = data['feature_names']\n",
    "y = data['target']\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   8.3252   37.88   -122.23  ]\n",
      " [   8.3014   37.86   -122.22  ]\n",
      " [   7.2574   37.85   -122.24  ]\n",
      " ...\n",
      " [   1.7      39.43   -121.22  ]\n",
      " [   1.8672   39.43   -121.32  ]\n",
      " [   2.3886   39.37   -121.24  ]]\n"
     ]
    }
   ],
   "source": [
    "#No need to create the Wrapper, scikit-learn already has implementation for RFE.\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#initialize the model\n",
    "LR_model = LinearRegression()\n",
    "\n",
    "#initialize RFE for above model with required number of feature(Top feature that we want to retain)\n",
    "\n",
    "rfe = RFE(estimator = LR_model,\n",
    "    n_features_to_select = 3\n",
    ")\n",
    "\n",
    "#fit RFE\n",
    "rfe.fit(X, y)\n",
    "\n",
    "#Getting the transfomed data with selected columns\n",
    "X_transformed = rfe.transform(X)\n",
    "\n",
    "#we can also use rfe.fit_transform(X, y) directly\n",
    "print(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above two approach are greedy method RFE, and greedy feature selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method - 5:(Model Based Selection)**\n",
    "   - Approach:\n",
    "       Selecting features based on ***Coefficients/importance of features(Tree based).***\n",
    "       \n",
    "       Procedure - we need to select the threshold, coeff > threshold then we keep the feature otherwise remove.\n",
    "   - 2nd approach can be using the model which support **L1 Norm(L1 regularizer(Lasso Penalization))** which create the sparsity i.e makes the most of the coefficient zero which are less important. And we select only feature with non-zero coefficient.  \n",
    "   \n",
    "**NOTE:**\n",
    "     * So in short we can get important features based on one model and then train another model based on features we had got from previous model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get all dependencies\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#load diabetes data for demonstration purpose\n",
    "data = load_diabetes()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "colm_names = data['feature_names']\n",
    "\n",
    "#initialize the model\n",
    "RF_model = RandomForestRegressor()\n",
    "RF_model.fit(X, y)\n",
    "\n",
    "#get the importance by using the coefficient \n",
    "importances = RF_model.feature_importances_\n",
    "\n",
    "#get indices according to the accending order of coefficient \n",
    "idxs = np.argsort(importances)\n",
    "\n",
    "plt.title(\"Features Imporatanaces\")\n",
    "plt.grid()\n",
    "plt.barh(range(len(idxs)), importances[idxs])\n",
    "plt.yticks(range(len(idxs)), [colm_names[i] for i in idxs])\n",
    "plt.ylabel('Columns')\n",
    "\n",
    "plt.xlabel('RandomForestFeature imporatance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method - 6:(Scikit-learn SelectFromModel class)**\n",
    "   - Approach:\n",
    "       idea is same as above but it will give the important features directly based on model that we've chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bmi', 's5']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "sfm = SelectFromModel(estimator = RF_model)\n",
    "X_transformed = sfm.fit_transform(X, y)\n",
    "support = sfm.get_support()\n",
    "\n",
    "#support return the True for those feature which are selected and False for those which are not selected\n",
    "\n",
    "#get feature names\n",
    "\n",
    "print([x for x, y in zip(colm_names, support) if y == True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From method - 5 we we can clearly see that we selected right features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using Sklearn Pipeline method with less code.(same thing achieve automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can also do same thing using sklern pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "#create dummy data\n",
    "X, y = make_classification(random_state=0)\n",
    "\n",
    "#split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "#creating pipeline object with feature selection and classification model\n",
    "pipe = Pipeline([('feature selection', SelectFromModel(LogisticRegression(penalty='l2'))), \n",
    "               ('classification', RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "#fit data into model\n",
    "pipe.fit(X, y)\n",
    "\n",
    "#get the score how good my model is?\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* here we are creating the model RandomForest based on selected features from LogisticRegression.\n",
    "* However in above we may overfit.\n",
    "* Order of tuples in pipe are very crucial.\n",
    "* But we are just reducing the code and precisely using the pipeline concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method - 7:(mlextend SequentialFeatureSelector wrapper)**\n",
    "   - Approach:\n",
    "       * it is same as method-4 greedy approach.\n",
    "       * it is exact forward feature selection \n",
    "       * mlxtend feature selector uses cross validation internally, and we set our desired folds.(More Robust Approach)\n",
    "       * Any method above is not guaranteed to get good results i.e AUC_score/accuracy then what we can do?\n",
    "           * Well, we could compare the resultant accuracies of the full model built using the selected features with the resultant accuracies of another full model using all of the features,\n",
    "           * if score not imporiving we change training algorithm to see how the selected feature performing diffrently.\n",
    "           * Such a feature selection method can be an effective part of a disciplined machine learning pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:   35.7s finished\n",
      "\n",
      "[2020-11-06 11:18:50] Features: 1/5 -- score: 0.7817733990147783[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:   35.3s finished\n",
      "\n",
      "[2020-11-06 11:19:26] Features: 2/5 -- score: 0.9647783251231526[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:   30.1s finished\n",
      "\n",
      "[2020-11-06 11:19:56] Features: 3/5 -- score: 0.9928571428571429[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.6s remaining:    0.0s\n"
     ]
    }
   ],
   "source": [
    "#data preparation for demontration purpose like fully actual project \n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_wine()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "X_train, x_test, Y_train, y_test = train_test_split(X, y , test_size = 0.2, random_state = 0)\n",
    "\n",
    "#now use model for forward selection\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "sfs = SFS(model,\n",
    "          k_features = 5,\n",
    "          forward = True,\n",
    "          floating = False,\n",
    "          verbose = 2,\n",
    "          scoring = \"accuracy\",\n",
    "          cv = 5)\n",
    "sfs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 4, 6, 8, 9, 10]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_features = list(sfs.k_feature_idx_)\n",
    "good_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_Accuracy :  0.9444444444444444\n",
      "Train_Accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "# Build full model with selected features\n",
    "# we can apply CV also based on hyperparametrs below is just a demonstration\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "\n",
    "#training\n",
    "clf = RandomForestClassifier(n_estimators = 200)\n",
    "clf.fit(X_train[:, good_features], Y_train)\n",
    "\n",
    "#testing the model\n",
    "y_test_pred = clf.predict(x_test[:, good_features])\n",
    "Y_train_pred = clf.predict(X_train[:, good_features])\n",
    "print('Test_Accuracy : ', acc(y_test, y_test_pred))\n",
    "print('Train_Accuracy : ', acc(Y_train, Y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_Accuracy :  0.9722222222222222\n",
      "Train_Accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "# Build full model on ALL features, for comparison\n",
    "#training\n",
    "clf = RandomForestClassifier(n_estimators = 200)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "#testing\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(x_test)\n",
    "\n",
    "#score\n",
    "print('Test_Accuracy : ', acc(y_test, y_test_pred))\n",
    "print('Train_Accuracy : ', acc(Y_train, Y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`Crucial Note Point for all above feature selection including current one`**:\n",
    "    \n",
    "*Keep in mind that step forward (or step backward(RFE)) methods, specifically, can provide problems **when dealing with especially large or highly-dimensional datasets.** There are ways of getting around (or trying to get around) these sticking points, such as **sampling from the data to find the feature subset which works best,** and then **using these features for the modeling process on the full dataset.** Of course, these are not the only disciplined approaches to feature selection either, and so checking out alternatives may be warranted when dealing with these larger datasets.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</h1><center>---FINISH---<center></h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
